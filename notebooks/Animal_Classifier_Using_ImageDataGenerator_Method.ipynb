{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Dn6Sl3suTg5"
   },
   "source": [
    "# Animal classifier usign CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iBnqTRp0FOe"
   },
   "source": [
    "## About Dataset and its structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2__rsBd40V5t"
   },
   "source": [
    "- I took this dataset from kaggle but this dataset official name is GTSRB - German Traffic Sign Recognition Benchmark, which had been rolled out by Germany.\n",
    "- This Dataset contains 43 types of traffic signs.\n",
    "- Structure of data folder\n",
    "\n",
    "\n",
    "![directory_structure.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OzXE2Ell0RBk"
   },
   "outputs": [],
   "source": [
    "classes = ['butterfly',\n",
    " 'cat',\n",
    " 'chicken',\n",
    " 'cow',\n",
    " 'dog',\n",
    " 'elephant',\n",
    " 'horse',\n",
    " 'sheep',\n",
    " 'spider',\n",
    " 'squirrel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vyhgm5Mu8mY"
   },
   "source": [
    "## 1. Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RDTWhXC8W2GP"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import shutil\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZLEpUUtwoMk"
   },
   "source": [
    "- Initializing some useful variables and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yGIBSV4SVvj3"
   },
   "outputs": [],
   "source": [
    "input_path = \"../data/archive/raw-img/\"\n",
    "\n",
    "\n",
    "image_data = []\n",
    "image_labels = []\n",
    "\n",
    "# Number of total classes\n",
    "total_classes = 10\n",
    "\n",
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Dimensions of our images\n",
    "height = 100\n",
    "width = 100\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMXFDVaPzm0u"
   },
   "source": [
    "### Spliting into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Z4VEQcOiyib7"
   },
   "outputs": [],
   "source": [
    "# train_data_gen = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     train_path,\n",
    "#     validation_split = 0.2,\n",
    "#     subset = \"training\",\n",
    "#     seed = 123,\n",
    "#     image_size = (height, width),\n",
    "#     batch_size = batch_size\n",
    "# )\n",
    "# val_data_gen = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     train_path,\n",
    "#     validation_split = 0.2,\n",
    "#     subset = \"validation\",\n",
    "#     seed = 123,\n",
    "#     image_size = (height, width),\n",
    "#     batch_size = batch_size\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5Gx9qTv8cKt"
   },
   "source": [
    "### Another method of doing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "TYmOBZA5zXK_"
   },
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                                                          validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class butterfly completed\n",
      "Class cat completed\n",
      "Class chicken completed\n",
      "Class cow completed\n",
      "Class dog completed\n",
      "Class elephant completed\n",
      "Class horse completed\n",
      "Class sheep completed\n",
      "Class spider completed\n",
      "Class squirrel completed\n"
     ]
    }
   ],
   "source": [
    "train_image_data = []\n",
    "train_image_labels = []\n",
    "for i in classes:\n",
    "    count = 0\n",
    "    path = input_path + i\n",
    "    images = os.listdir(path)\n",
    "    for img in images:\n",
    "        temp = cv2.imread(path + \"/\" + img)\n",
    "        temp = Image.fromarray(temp, 'RGB')\n",
    "        temp = temp.resize((height, width))\n",
    "        train_image_data.append(np.array(temp))\n",
    "        train_image_labels.append(i)\n",
    "    print(f\"Class {i} completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = np.array(train_image_data)\n",
    "image_labels = np.array(train_image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26179, 100, 100, 3)\n",
      "(26179,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_image_data))\n",
    "print(np.shape(train_image_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez_compressed(\"../data/data.npz\", train_image_data, train_image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(image_data, image_labels,\n",
    "                                                    stratify=train_image_labels,\n",
    "                                                    test_size=0.3\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'horse': 1836, 'spider': 3375, 'dog': 3404, 'chicken': 2169, 'cow': 1306, 'squirrel': 1303, 'sheep': 1274, 'butterfly': 1478, 'elephant': 1012, 'cat': 1168}\n",
      "{'butterfly': 634, 'sheep': 546, 'chicken': 929, 'cat': 500, 'spider': 1446, 'horse': 787, 'cow': 560, 'dog': 1459, 'elephant': 434, 'squirrel': 559}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(dict(Counter(y_train)))\n",
    "print(dict(Counter(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GvqLPo1Q7HLc",
    "outputId": "79ced076-f3f7-43a3-b6c2-75e5741622ec"
   },
   "outputs": [],
   "source": [
    "train_datagen = datagen.flow(X_train, y_train,\n",
    "                            batch_size = batch_size,\n",
    "                            subset = \"training\",\n",
    "                           shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K2ehWtQa7byz",
    "outputId": "9f903f24-4d77-44e0-a8bf-b561c838f69f"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "flow() got an unexpected keyword argument 'target_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-979da6abb05f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m val_datagen = datagen.flow(X_test, y_test,\n\u001b[0m\u001b[1;32m      2\u001b[0m                             \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: flow() got an unexpected keyword argument 'target_size'"
     ]
    }
   ],
   "source": [
    "val_datagen = datagen.flow(X_test, y_test,\n",
    "                            target_size = (32, 32),\n",
    "                            batch_size = batch_size,\n",
    "                            subset = \"validation\",\n",
    "                            class_mode='categorical',\n",
    "                           shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WJOpnP78m64"
   },
   "source": [
    "## Creating model architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YmAV_rKc45nx"
   },
   "outputs": [],
   "source": [
    "def my_model():\n",
    "    input_ = tf.keras.layers.Input((32, 32, 3))\n",
    "    \n",
    "    net = tf.keras.layers.Conv2D(16, (3, 3), strides=(1, 1), padding='valid', activation='relu')(input_)\n",
    "    net = tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(net)\n",
    "#     net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    \n",
    "    net = tf.keras.layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same', activation='relu')(net)\n",
    "    net = tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(net)\n",
    "#     net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    \n",
    "    net = tf.keras.layers.Conv2D(64, (7, 7), strides=(1, 1), padding='same', activation='relu')(net)\n",
    "    net = tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(net)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    \n",
    "    net = tf.keras.layers.Flatten()(net)\n",
    "    net = tf.keras.layers.Dense(72, activation = 'relu', kernel_regularizer=keras.regularizers.l1_l2(l1=0.1, l2=0.01))(net)\n",
    "    net = tf.keras.layers.Dense(36, activation = 'relu', kernel_regularizer=keras.regularizers.l1_l2(l1=0.1, l2=0.01))(net)\n",
    "    out = tf.keras.layers.Dense(10, activation='softmax')(net)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_], outputs=[out])\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy, \n",
    "                  optimizer=tf.keras.optimizers.Adam(lr=0.003), \n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model, to_file='./model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EuLj_lWu_HEs",
    "outputId": "c94252d0-10d8-4bb4-f3ff-db601c6cdbca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 16)        448       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_5 (Average (None, 15, 15, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 32)          12832     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_6 (Average (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 64)          100416    \n",
      "_________________________________________________________________\n",
      "average_pooling2d_7 (Average (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 72)                18504     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 36)                2628      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                370       \n",
      "=================================================================\n",
      "Total params: 135,198\n",
      "Trainable params: 135,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = my_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqvIEiAKAJ7Y"
   },
   "source": [
    "### Defining Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-toQrJdA_KoE"
   },
   "outputs": [],
   "source": [
    "# Define a Callback class that stops training once accuracy reaches certain limit\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('val_accuracy')>0.90 and logs.get('accuracy')>0.93):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "        \n",
    "reduceLROnPlat = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.95,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        mode='min',\n",
    "        min_delta=0.0001,\n",
    "        cooldown=2,\n",
    "        min_lr=1e-5\n",
    ")\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"../model/model_best.h5\", monitor='val_loss', verbose=1, save_best_only=False,\n",
    "    save_weights_only=False, mode='auto', save_freq='epoch'\n",
    ")\n",
    "\n",
    "my_callback = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQ6Q_pJPE47q",
    "outputId": "44e096a8-f7af-4e8b-bad3-d1f4fb067451"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 76/328 [=====>........................] - ETA: 16s - loss: 41.8539 - accuracy: 0.1900"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-400e1d53b56a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit_generator(train_datagen,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_datagen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                              callbacks=[reduceLROnPlat, my_callback, checkpoint])\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1941\u001b[0m                   \u001b[0;34m'will be removed in a future version. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m                   'Please use `Model.fit`, which supports generators.')\n\u001b[0;32m-> 1943\u001b[0;31m     return self.fit(\n\u001b[0m\u001b[1;32m   1944\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_datagen,\n",
    "                               epochs = 50,\n",
    "                               verbose = 1,\n",
    "                               validation_data = val_datagen,\n",
    "                             callbacks=[reduceLROnPlat, my_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"../model/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = \"../data/archive/raw-img/horse/OIP--jy2_p3NvNX51l-FNfOPzAHaFj.jpeg\"\n",
    "def get_output(img):\n",
    "    img1 = tf.keras.preprocessing.image.load_img(img, target_size=(32, 32), interpolation='bicubic')\n",
    "    test_img = tf.keras.preprocessing.image.img_to_array(img1, data_format=\"channels_last\")/255\n",
    "    test_img = np.expand_dims(test_img, axis=0)\n",
    "    scores = model.predict(test_img)\n",
    "    prob = np.max(scores, axis = 1)\n",
    "    preds = np.argmax(scores, axis = 1)\n",
    "    print(f\"{prob[0] * 100}% probability of {classes[preds[0]]}\")\n",
    "get_output(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1123ynKEuvVl",
    "LaNayEiYrnMF"
   ],
   "name": "Recognizing_Traffic_Signs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
